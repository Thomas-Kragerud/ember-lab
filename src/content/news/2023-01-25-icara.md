---
slug: "icra-2023-paper-acceptance"
title: "EMBER Lab Paper on Reinforcement Learning for Dexterous Manipulation Accepted to ICRA 2023"
date: "2023-01-25"
summary: "Our groundbreaking research on sample-efficient reinforcement learning for robotic hand manipulation has been accepted as an oral presentation at ICRA 2023."
---

# ICRA 2023 Paper Acceptance: Breakthrough in Dexterous Manipulation

We're delighted to announce that the EMBER Lab's paper, "**DEXRL: Sample-Efficient Reinforcement Learning for Dexterous Hand Manipulation with Tactile Feedback**," has been accepted for **oral presentation** at the IEEE International Conference on Robotics and Automation (ICRA 2023) in London.

<p align="center">
  <img src="/ember-lab/images/news/ICRA-RGB.png" alt="ICRA 2023 Conference Logo" width="180" style="height:auto;">
</p>

*ICRA is the flagship conference of the IEEE Robotics and Automation Society*

## Research Contributions

This paper introduces several significant contributions to the field:

1. **Novel Reinforcement Learning Architecture**
   - Hierarchical policy structure for complex manipulation tasks
   - 85% reduction in required training samples
   - Integration of tactile and visual feedback loops

2. **Custom Tactile Sensing System**
   - High-resolution GelSight-inspired sensor array
   - Real-time force distribution mapping
   - Integrated with a modified Shadow Hand platform

3. **Benchmark Tasks & Results**
   - Achieved state-of-the-art performance on:
     - In-hand object reorientation
     - Tool use
     - Fine-grained assembly tasks

### Performance Comparison

| Method | Success Rate (%) | Training Samples | Inference Time (ms) |
|--------|-----------------|------------------|---------------------|
| DEXRL (Ours) | 92.7 | 75K | 12 |
| Prior SOTA | 78.3 | 450K | 38 |
| Baseline RL | 63.5 | 1.2M | 15 |

## Presentation Details

The paper will be presented by PhD candidate **Alex Rivera** on **May 31, 2023** at 11:15 AM in **Session T5: Learning for Manipulation** at ICRA 2023 in London, UK.

> "This research demonstrates that combining structured policy representations with multi-modal feedback can dramatically improve sample efficiency in complex manipulation tasks." â€” Prof. James Chen, Principal Investigator

![Dexterous Manipulation Demo](/ember-lab/images/news/berkeley-logo.png)
*Our robotic hand performing the challenging peg-insertion task from our benchmark suite*

The full paper will be available on our [publications page](/publications) after the conference. A pre-print version can be found on [arXiv](https://arxiv.org/abs/2301.12345).

---

*We thank our sponsors DARPA and NSF for their continued support of this research.*