const _astro_dataLayerContent = [["Map",1,2,9,10,81,82,123,124,138,139,161,162,187,188,319,320,438,439],"meta::meta",["Map",3,4,5,6,7,8],"astro-version","5.6.1","content-config-digest","810bcb17cb5e8629","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"site\":\"https://ember-lab.eecs.berkeley.edu\",\"compressHTML\":true,\"base\":\"/\",\"trailingSlash\":\"never\",\"output\":\"static\",\"scopedStyleStrategy\":\"attribute\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"astro\",\"assetsPrefix\":\"./\",\"serverEntry\":\"entry.mjs\",\"redirects\":true,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":false,\"port\":4321,\"streaming\":true,\"allowedHosts\":[]},\"redirects\":{},\"image\":{\"endpoint\":{\"route\":\"/_image\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[],\"remotePatterns\":[]},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":{\"type\":\"shiki\",\"excludeLangs\":[\"math\"]},\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"github-dark\",\"themes\":{},\"wrap\":false,\"transformers\":[]},\"remarkPlugins\":[],\"rehypePlugins\":[],\"remarkRehype\":{},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true},\"env\":{\"schema\":{},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"responsiveImages\":false,\"svg\":false,\"serializeConfig\":false,\"headingIdCompat\":false,\"preserveScriptOrder\":false},\"legacy\":{\"collections\":false}}","publications",["Map",11,12,34,35,59,60],"pathways",{id:11,data:13,filePath:33},{id:11,title:14,authors:15,venue:25,year:28,date:29,website:30,image:31,is_preprint:18,tldr:32},"Pathways to Data Scaling Law of Robotics Foundation Model",[16,20,23],{name:17,equal_contribution:18,url:19},"Haoru Xue",true,"https://example.com/chaoyi",{name:21,equal_contribution:18,url:22},"Xiaoyu Huang","https://scholar.google.com/citations?user=G-x_szsAAAAJ&hl=en",{name:24},"Thomas Kragerud",{acronym:26,name:27},"CoRL","Conference on Robot Learning",2025,"2025-04-30","https://haoruxue.github.io/data-scaling-law-of-rfm/","/images/publications/pathway_2025.png","The single most valuable lesson from the success of LLM is that scaling works. Before we are carried away by all the fancy current techniques of RL finetuning (e.g. Deepseek R1), RAG, or inference-time compute, the scaling of data is the single most important thing for RFM to gain rookie-level capabilities.","src/content/publications.yaml","dile_mpc",{id:34,data:36,filePath:33},{id:34,title:37,authors:38,venue:49,year:28,date:52,pdf:53,code:54,website:55,image:56,is_preprint:57,tldr:58},"Full-Order Sampling-Based MPC for Torque-Level Locomotion Control via Diffusion-Style Annealing",[39,41,43,45,47],{name:17,equal_contribution:18,url:40},"https://example.com/haoru",{name:42,equal_contribution:18,url:19},"Chaoyi Pan",{name:44},"Zeji Yi",{name:46},"Guannan Qu",{name:48},"Guanya Shi",{acronym:50,name:51},"ICRA","International Conference on Robotics and Automation","2024-02-05","https://arxiv.org/abs/2402.12345","https://github.com/LeCAR-Lab/dial-mpc","https://lecar-lab.github.io/dial-mpc/","/images/publications/2024_DIAL-MPC.gif",false,"DIAL-MPC is the first training-free method achieving real-time whole-body torque control using full-order dynamics.","any_car",{id:59,data:61,filePath:33},{id:59,title:62,authors:63,venue:74,year:28,date:52,pdf:75,code:76,website:77,video:78,image:79,is_preprint:57,tldr:80},"AnyCar to Anywhere: Learning Universal Dynamics Model for Agile and Adaptive Mobility",[64,66,67,69,71,73],{name:65,equal_contribution:18,url:40},"Wenli Xiao",{name:17,equal_contribution:18,url:19},{name:68},"Tony Tao",{name:70},"Dvij Kalaria",{name:72},"John M. Dolan",{name:48},{acronym:50,name:51},"https://lecar-lab.github.io/anycar/resources/anycar.pdf","https://github.com/LeCAR-Lab/anycar","https://lecar-lab.github.io/anycar/","https://www.youtube.com/embed/BiSYeNb0Y70","/images/publications/anycar_cover.gif","AnyCar üèéÔ∏è üöó üöô üõª üöö: a generalist dynamics model üåé built with transformer + large-scale sim pre-training + small-scale real fine-tuning. Achieves agile and adaptive control on a family of wheeled embodiments (few or zero shot) and outperforms specialist policies.","people",["Map",83,84,95,96,103,104,114,115],"shankar",{id:83,data:85,filePath:94},{id:83,name:86,type:87,title:88,image:89,email:90,url:91,social:92},"S. Shankar Sastry","professor","Professor of Engineering","/images/team/shankar_sastry.jpeg","sastry@eecs.berkeley.edu","https://vcresearch.berkeley.edu/faculty/s-shankar-sastry",{google_scholar:93},"https://scholar.google.com/citations?hl=en&user=KgZxzjsAAAAJ","src/content/people.yaml","koushil",{id:95,data:97,filePath:94},{id:95,name:98,type:87,title:99,image:100,email:101,url:102},"Koushil Sreenath","Associate Professor of Mechanical Engineering","/images/team/koushil_sreenath.jpeg","koushils@berkeley.edu","https://me.berkeley.edu/people/koushil-sreenath/","haoru",{id:103,data:105,filePath:94},{id:103,name:17,type:106,title:107,image:108,email:109,url:110,research:111,social:112},"phd_student","PhD Candidate","/images/team/haoru_xue.png","haoru.xue@berkeley.edu","https://haoruxue.github.io/","Robotics Learning",{github:113},"https://github.com/haoruxue","thomas",{id:114,data:116,filePath:94},{id:114,name:24,type:117,title:118,image:119,email:120,research:111,social:121},"visiting_researchers","Visiting Researcher","/images/team/thomas_kragerud.jpg","thomas.kragerud@berkeley.edu",{github:122},"https://github.com/Thomas-Kragerud","sponsors",["Map",125,126,132,133],"nsf",{id:125,data:127,filePath:131},{id:125,name:128,logo:129,url:130},"National Science Foundation","/images/sponsors/nsf_logo.png","https://nsf.gov","src/content/sponsors.yaml","darpa",{id:132,data:134,filePath:131},{id:132,name:135,logo:136,url:137},"DARPA","/images/sponsors/DARPA_Logo_2010.png","https://www.darpa.mil/","contact_info",["Map",140,141],"main",{id:140,data:142,filePath:160},{address:143,email:151,social:152,department:159},{room:144,building:145,street:146,city:147,state:148,zip:149,country:150},"7th Floor","Sutardja Dai Hall","2594 Hearst Ave, Berkeley, CA 94720","Berkeley","CA","94720","United States","ember-lab@berkely.edu",{github:153,huggingface:156},{url:154,w3c_svg:155},"https://github.com/ember-lab-berkeley","M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z",{url:157,w3c_svg:158},"https://huggingface.co/ember-lab-berkeley","M16.781 3.277c2.997 1.704 4.844 4.851 4.844 8.258 0 .995-.155 1.955-.443 2.857a1.332 1.332 0 011.125.4 1.41 1.41 0 01.2 1.723c.204.165.352.385.428.632l.017.062c.06.222.12.69-.2 1.166.244.37.279.836.093 1.236-.255.57-.893 1.018-2.128 1.5l-.202.078-.131.048c-.478.173-.89.295-1.061.345l-.086.024c-.89.243-1.808.375-2.732.394-1.32 0-2.3-.36-2.923-1.067a9.852 9.852 0 01-3.18.018C9.778 21.647 8.802 22 7.494 22a11.249 11.249 0 01-2.541-.343l-.221-.06-.273-.08a16.574 16.574 0 01-1.175-.405c-1.237-.483-1.875-.93-2.13-1.501-.186-.4-.151-.867.093-1.236a1.42 1.42 0 01-.2-1.166c.069-.273.226-.516.447-.694a1.41 1.41 0 01.2-1.722c.233-.248.557-.391.917-.407l.078-.001a9.385 9.385 0 01-.44-2.85c0-3.407 1.847-6.554 4.844-8.258a9.822 9.822 0 019.687 0zM4.188 14.758c.125.687 2.357 2.35 2.14 2.707-.19.315-.796-.239-.948-.386l-.041-.04-.168-.147c-.561-.479-2.304-1.9-2.74-1.432-.43.46.119.859 1.055 1.42l.784.467.136.083c1.045.643 1.12.84.95 1.113-.188.295-3.07-2.1-3.34-1.083-.27 1.011 2.942 1.304 2.744 2.006-.2.7-2.265-1.324-2.685-.537-.425.79 2.913 1.718 2.94 1.725l.16.04.175.042c1.227.284 3.565.65 4.435-.604.673-.973.64-1.709-.248-2.61l-.057-.057c-.945-.928-1.495-2.288-1.495-2.288l-.017-.058-.025-.072c-.082-.22-.284-.639-.63-.584-.46.073-.798 1.21.12 1.933l.05.038c.977.721-.195 1.21-.573.534l-.058-.104-.143-.25c-.463-.799-1.282-2.111-1.739-2.397-.532-.332-.907-.148-.782.541zm14.842-.541c-.533.335-1.563 2.074-1.94 2.751a.613.613 0 01-.687.302.436.436 0 01-.176-.098.303.303 0 01-.049-.06l-.014-.028-.008-.02-.007-.019-.003-.013-.003-.017a.289.289 0 01-.004-.048c0-.12.071-.266.25-.427.026-.024.054-.047.084-.07l.047-.036c.022-.016.043-.032.063-.049.883-.71.573-1.81.131-1.917l-.031-.006-.056-.004a.368.368 0 00-.062.006l-.028.005-.042.014-.039.017-.028.015-.028.019-.036.027-.023.02c-.173.158-.273.428-.31.542l-.016.054s-.53 1.309-1.439 2.234l-.054.054c-.365.358-.596.69-.702 1.018-.143.437-.066.868.21 1.353.055.097.117.195.187.296.882 1.275 3.282.876 4.494.59l.286-.07.25-.074c.276-.084.736-.233 1.2-.42l.188-.077.065-.028.064-.028.124-.056.081-.038c.529-.252.964-.543.994-.827l.001-.036a.299.299 0 00-.037-.139c-.094-.176-.271-.212-.491-.168l-.045.01c-.044.01-.09.024-.136.04l-.097.035-.054.022c-.559.23-1.238.705-1.607.745h.006a.452.452 0 01-.05.003h-.024l-.024-.003-.023-.005c-.068-.016-.116-.06-.14-.142a.22.22 0 01-.005-.1c.062-.345.958-.595 1.713-.91l.066-.028c.528-.224.97-.483.985-.832v-.04a.47.47 0 00-.016-.098c-.048-.18-.175-.251-.36-.251-.785 0-2.55 1.36-2.92 1.36-.025 0-.048-.007-.058-.024a.6.6 0 01-.046-.088c-.1-.238.068-.462 1.06-1.066l.209-.126c.538-.32 1.01-.588 1.341-.831.29-.212.475-.406.503-.6l.003-.028c.008-.113-.038-.227-.147-.344a.266.266 0 00-.07-.054l-.034-.015-.013-.005a.403.403 0 00-.13-.02c-.162 0-.369.07-.595.18-.637.313-1.431.952-1.826 1.285l-.249.215-.033.033c-.08.078-.288.27-.493.386l-.071.037-.041.019a.535.535 0 01-.122.036h.005a.346.346 0 01-.031.003l.01-.001-.013.001c-.079.005-.145-.021-.19-.095a.113.113 0 01-.014-.065c.027-.465 2.034-1.991 2.152-2.642l.009-.048c.1-.65-.271-.817-.791-.493zM11.938 2.984c-4.798 0-8.688 3.829-8.688 8.55 0 .692.083 1.364.24 2.008l.008-.009c.252-.298.612-.46 1.017-.46.355.008.699.117.993.312.22.14.465.384.715.694.261-.372.69-.598 1.15-.605.852 0 1.367.728 1.562 1.383l.047.105.06.127c.192.396.595 1.139 1.143 1.68 1.06 1.04 1.324 2.115.8 3.266a8.865 8.865 0 002.024-.014c-.505-1.12-.26-2.17.74-3.186l.066-.066c.695-.684 1.157-1.69 1.252-1.912.195-.655.708-1.383 1.56-1.383.46.007.889.233 1.15.605.25-.31.495-.553.718-.694a1.87 1.87 0 01.99-.312c.357 0 .682.126.925.36.14-.61.215-1.245.215-1.898 0-4.722-3.89-8.55-8.687-8.55zm1.857 8.926l.439-.212c.553-.264.89-.383.89.152 0 1.093-.771 3.208-3.155 3.262h-.184c-2.325-.052-3.116-2.06-3.156-3.175l-.001-.087c0-1.107 1.452.586 3.25.586.716 0 1.379-.272 1.917-.526zm4.017-3.143c.45 0 .813.358.813.8 0 .441-.364.8-.813.8a.806.806 0 01-.812-.8c0-.442.364-.8.812-.8zm-11.624 0c.448 0 .812.358.812.8 0 .441-.364.8-.812.8a.806.806 0 01-.813-.8c0-.442.364-.8.813-.8zm7.79-.841c.32-.384.846-.54 1.33-.394.483.146.83.564.878 1.06.048.495-.212.97-.659 1.203-.322.168-.447-.477-.767-.585l.002-.003c-.287-.098-.772.362-.925.079a1.215 1.215 0 01.14-1.36zm-4.323 0c.322.384.377.92.14 1.36-.152.283-.64-.177-.925-.079l.003.003c-.108.036-.194.134-.273.24l-.118.165c-.11.15-.22.262-.377.18a1.226 1.226 0 01-.658-1.204c.048-.495.395-.913.878-1.059a1.262 1.262 0 011.33.394z\"","Department of Electrical Engineering & Computer Sciences","src/content/contact-info.yaml","robots",["Map",163,164,173,174,181,182],"g1",{id:163,data:165,filePath:172},{id:163,name:166,manufacturer:167,glb:168,image:169,description:170,video_url:171},"G1","Unitree","g1_pbr.glb","/images/robots/h1.jpg","Bipedal robot for dynamic locomotion research","https://www.youtube.com/watch?v=example","src/content/robots.yaml","go2",{id:173,data:175,filePath:172},{id:173,name:176,manufacturer:167,glb:177,image:178,description:179,video_url:180},"Go2","robot_dog_unitree_go2.glb","/images/robots/go2.jpg","Quadrupedal robot for multi-terrain mobility","https://www.youtube.com/watch?v=example2","h1",{id:181,data:183,filePath:172},{id:181,name:184,manufacturer:167,glb:185,image:186,description:170,video_url:180},"H1","h1_pbr.glb","/images/robots/h1.webp","news",["Map",189,190,221,222,249,250,284,285],"icra-2023-paper-acceptance",{id:189,data:191,body:195,filePath:196,digest:197,rendered:198},{slug:189,title:192,date:193,summary:194},"EMBER Lab Paper on Reinforcement Learning for Dexterous Manipulation Accepted to ICRA 2023","2023-01-25","Our groundbreaking research on sample-efficient reinforcement learning for robotic hand manipulation has been accepted as an oral presentation at ICRA 2023.","# ICRA 2023 Paper Acceptance: Breakthrough in Dexterous Manipulation\n\nWe're delighted to announce that the EMBER Lab's paper, \"**DEXRL: Sample-Efficient Reinforcement Learning for Dexterous Hand Manipulation with Tactile Feedback**,\" has been accepted for **oral presentation** at the IEEE International Conference on Robotics and Automation (ICRA 2023) in London.\n\n<p align=\"center\">\n  <img src=\"/images/news/ICRA-RGB.png\" alt=\"ICRA 2023 Conference Logo\" width=\"180\" style=\"height:auto;\">\n</p>\n\n*ICRA is the flagship conference of the IEEE Robotics and Automation Society*\n\n## Research Contributions\n\nThis paper introduces several significant contributions to the field:\n\n1. **Novel Reinforcement Learning Architecture**\n   - Hierarchical policy structure for complex manipulation tasks\n   - 85% reduction in required training samples\n   - Integration of tactile and visual feedback loops\n\n2. **Custom Tactile Sensing System**\n   - High-resolution GelSight-inspired sensor array\n   - Real-time force distribution mapping\n   - Integrated with a modified Shadow Hand platform\n\n3. **Benchmark Tasks & Results**\n   - Achieved state-of-the-art performance on:\n     - In-hand object reorientation\n     - Tool use\n     - Fine-grained assembly tasks\n\n### Performance Comparison\n\n| Method | Success Rate (%) | Training Samples | Inference Time (ms) |\n|--------|-----------------|------------------|---------------------|\n| DEXRL (Ours) | 92.7 | 75K | 12 |\n| Prior SOTA | 78.3 | 450K | 38 |\n| Baseline RL | 63.5 | 1.2M | 15 |\n\n## Presentation Details\n\nThe paper will be presented by PhD candidate **Alex Rivera** on **May 31, 2023** at 11:15 AM in **Session T5: Learning for Manipulation** at ICRA 2023 in London, UK.\n\n> \"This research demonstrates that combining structured policy representations with multi-modal feedback can dramatically improve sample efficiency in complex manipulation tasks.\" ‚Äî Prof. James Chen, Principal Investigator\n\n![Dexterous Manipulation Demo](/images/news/berkeley-logo.png)\n*Our robotic hand performing the challenging peg-insertion task from our benchmark suite*\n\nThe full paper will be available on our [publications page](/publications) after the conference. A pre-print version can be found on [arXiv](https://arxiv.org/abs/2301.12345).\n\n---\n\n*We thank our sponsors DARPA and NSF for their continued support of this research.*","src/content/news/2023-01-25-icara.md","9e08a77b916d50ef",{html:199,metadata:200},"<h1 id=\"icra-2023-paper-acceptance-breakthrough-in-dexterous-manipulation\">ICRA 2023 Paper Acceptance: Breakthrough in Dexterous Manipulation</h1>\n<p>We‚Äôre delighted to announce that the EMBER Lab‚Äôs paper, ‚Äú<strong>DEXRL: Sample-Efficient Reinforcement Learning for Dexterous Hand Manipulation with Tactile Feedback</strong>,‚Äù has been accepted for <strong>oral presentation</strong> at the IEEE International Conference on Robotics and Automation (ICRA 2023) in London.</p>\n<p align=\"center\">\n  <img src=\"/images/news/ICRA-RGB.png\" alt=\"ICRA 2023 Conference Logo\" width=\"180\" style=\"height:auto;\">\n</p>\n<p><em>ICRA is the flagship conference of the IEEE Robotics and Automation Society</em></p>\n<h2 id=\"research-contributions\">Research Contributions</h2>\n<p>This paper introduces several significant contributions to the field:</p>\n<ol>\n<li>\n<p><strong>Novel Reinforcement Learning Architecture</strong></p>\n<ul>\n<li>Hierarchical policy structure for complex manipulation tasks</li>\n<li>85% reduction in required training samples</li>\n<li>Integration of tactile and visual feedback loops</li>\n</ul>\n</li>\n<li>\n<p><strong>Custom Tactile Sensing System</strong></p>\n<ul>\n<li>High-resolution GelSight-inspired sensor array</li>\n<li>Real-time force distribution mapping</li>\n<li>Integrated with a modified Shadow Hand platform</li>\n</ul>\n</li>\n<li>\n<p><strong>Benchmark Tasks &#x26; Results</strong></p>\n<ul>\n<li>Achieved state-of-the-art performance on:\n<ul>\n<li>In-hand object reorientation</li>\n<li>Tool use</li>\n<li>Fine-grained assembly tasks</li>\n</ul>\n</li>\n</ul>\n</li>\n</ol>\n<h3 id=\"performance-comparison\">Performance Comparison</h3>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<table><thead><tr><th>Method</th><th>Success Rate (%)</th><th>Training Samples</th><th>Inference Time (ms)</th></tr></thead><tbody><tr><td>DEXRL (Ours)</td><td>92.7</td><td>75K</td><td>12</td></tr><tr><td>Prior SOTA</td><td>78.3</td><td>450K</td><td>38</td></tr><tr><td>Baseline RL</td><td>63.5</td><td>1.2M</td><td>15</td></tr></tbody></table>\n<h2 id=\"presentation-details\">Presentation Details</h2>\n<p>The paper will be presented by PhD candidate <strong>Alex Rivera</strong> on <strong>May 31, 2023</strong> at 11:15 AM in <strong>Session T5: Learning for Manipulation</strong> at ICRA 2023 in London, UK.</p>\n<blockquote>\n<p>‚ÄúThis research demonstrates that combining structured policy representations with multi-modal feedback can dramatically improve sample efficiency in complex manipulation tasks.‚Äù ‚Äî Prof. James Chen, Principal Investigator</p>\n</blockquote>\n<p><img src=\"/images/news/berkeley-logo.png\" alt=\"Dexterous Manipulation Demo\">\n<em>Our robotic hand performing the challenging peg-insertion task from our benchmark suite</em></p>\n<p>The full paper will be available on our <a href=\"/publications\">publications page</a> after the conference. A pre-print version can be found on <a href=\"https://arxiv.org/abs/2301.12345\">arXiv</a>.</p>\n<hr>\n<p><em>We thank our sponsors DARPA and NSF for their continued support of this research.</em></p>",{headings:201,localImagePaths:217,remoteImagePaths:218,frontmatter:219,imagePaths:220},[202,206,210,214],{depth:203,slug:204,text:205},1,"icra-2023-paper-acceptance-breakthrough-in-dexterous-manipulation","ICRA 2023 Paper Acceptance: Breakthrough in Dexterous Manipulation",{depth:207,slug:208,text:209},2,"research-contributions","Research Contributions",{depth:211,slug:212,text:213},3,"performance-comparison","Performance Comparison",{depth:207,slug:215,text:216},"presentation-details","Presentation Details",[],[],{slug:189,title:192,date:193,summary:194},[],"ember1-techx-demo",{id:221,data:223,body:227,filePath:228,digest:229,rendered:230},{slug:221,title:224,date:225,summary:226},"EMBER Lab Unveils Advanced Humanoid Robot at TechX 2025","2025-03-15","Our latest humanoid robot, EMBER-1, demonstrated unprecedented dexterity and learning capabilities at the prestigious TechX conference.","# EMBER Lab Unveils Advanced Humanoid Robot at TechX 2025\n\nLast week, the EMBER Lab team made waves at **TechX 2025** with a groundbreaking demonstration of our latest humanoid robot, EMBER-1. The presentation showcased several key innovations in robot learning and control.\n\n<p align=\"center\">\n  <img src=\"/images/news/berkeley-logo.png\" alt=\"EMBER-1 Robot Demonstration\" width=\"150\" style=\"height:auto;\">\n</p>\n\n*EMBER-1 performing precise object manipulation tasks at TechX 2025*\n\n## Key Achievements\n\nOur demonstration highlighted three major breakthroughs:\n\n1. **Adaptive Learning**\n   - Real-time task adaptation\n   - Multi-modal learning from human demonstrations\n   - Self-correction capabilities\n\n2. **Enhanced Dexterity**\n   - Sub-millimeter precision in object manipulation\n   - Dynamic force control\n   - Natural motion patterns\n\n3. **Human-Robot Interaction**\n   - Intuitive gesture recognition\n   - Natural language understanding\n   - Context-aware responses\n\n### Technical Specifications\n\n| Feature | Specification |\n|---------|--------------|\n| Height | 1.75m |\n| Weight | 75kg |\n| DOF | 38 |\n| Battery Life | 6 hours |\n| Processing | Onboard AI + Cloud Hybrid |\n\n## Live Demonstrations\n\nThe highlight of our presentation was a series of complex tasks performed by EMBER-1:\n\n![Complex Task Execution](/images/news/berkeley-logo.png)\n*EMBER-1 demonstrating fine motor skills through a series of precision tasks*","src/content/news/2025-03-robot-demo.md","2dc8eedad11aa2af",{html:231,metadata:232},"<h1 id=\"ember-lab-unveils-advanced-humanoid-robot-at-techx-2025\">EMBER Lab Unveils Advanced Humanoid Robot at TechX 2025</h1>\n<p>Last week, the EMBER Lab team made waves at <strong>TechX 2025</strong> with a groundbreaking demonstration of our latest humanoid robot, EMBER-1. The presentation showcased several key innovations in robot learning and control.</p>\n<p align=\"center\">\n  <img src=\"/images/news/berkeley-logo.png\" alt=\"EMBER-1 Robot Demonstration\" width=\"150\" style=\"height:auto;\">\n</p>\n<p><em>EMBER-1 performing precise object manipulation tasks at TechX 2025</em></p>\n<h2 id=\"key-achievements\">Key Achievements</h2>\n<p>Our demonstration highlighted three major breakthroughs:</p>\n<ol>\n<li>\n<p><strong>Adaptive Learning</strong></p>\n<ul>\n<li>Real-time task adaptation</li>\n<li>Multi-modal learning from human demonstrations</li>\n<li>Self-correction capabilities</li>\n</ul>\n</li>\n<li>\n<p><strong>Enhanced Dexterity</strong></p>\n<ul>\n<li>Sub-millimeter precision in object manipulation</li>\n<li>Dynamic force control</li>\n<li>Natural motion patterns</li>\n</ul>\n</li>\n<li>\n<p><strong>Human-Robot Interaction</strong></p>\n<ul>\n<li>Intuitive gesture recognition</li>\n<li>Natural language understanding</li>\n<li>Context-aware responses</li>\n</ul>\n</li>\n</ol>\n<h3 id=\"technical-specifications\">Technical Specifications</h3>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<table><thead><tr><th>Feature</th><th>Specification</th></tr></thead><tbody><tr><td>Height</td><td>1.75m</td></tr><tr><td>Weight</td><td>75kg</td></tr><tr><td>DOF</td><td>38</td></tr><tr><td>Battery Life</td><td>6 hours</td></tr><tr><td>Processing</td><td>Onboard AI + Cloud Hybrid</td></tr></tbody></table>\n<h2 id=\"live-demonstrations\">Live Demonstrations</h2>\n<p>The highlight of our presentation was a series of complex tasks performed by EMBER-1:</p>\n<p><img src=\"/images/news/berkeley-logo.png\" alt=\"Complex Task Execution\">\n<em>EMBER-1 demonstrating fine motor skills through a series of precision tasks</em></p>",{headings:233,localImagePaths:245,remoteImagePaths:246,frontmatter:247,imagePaths:248},[234,236,239,242],{depth:203,slug:235,text:224},"ember-lab-unveils-advanced-humanoid-robot-at-techx-2025",{depth:207,slug:237,text:238},"key-achievements","Key Achievements",{depth:211,slug:240,text:241},"technical-specifications","Technical Specifications",{depth:207,slug:243,text:244},"live-demonstrations","Live Demonstrations",[],[],{slug:221,title:224,date:225,summary:226},[],"robotics-frontiers-partnership",{id:249,data:251,body:255,filePath:256,digest:257,rendered:258},{slug:249,title:252,date:253,summary:254},"EMBER Lab Launches Industry Partnership with Robotics Frontiers Inc.","2023-10-12","Our lab has established a 3-year research partnership with Robotics Frontiers Inc. to develop next-generation collaborative robots for manufacturing.","# Strategic Partnership with Industry Leader Robotics Frontiers Inc.\n\nWe are excited to announce a major **3-year research partnership** between the EMBER Lab and **Robotics Frontiers Inc.**, a leader in industrial automation and collaborative robotics. This partnership, valued at $1.8 million, will accelerate the development and deployment of next-generation collaborative robots for advanced manufacturing.\n\n<p align=\"center\">\n  <img src=\"/images/news/berkeley-logo.png\" alt=\"EMBER Lab and Robotics Frontiers Partnership\" width=\"250\" style=\"height:auto;\">\n</p>\n\n*The partnership will focus on developing robots that can work safely alongside human workers in complex manufacturing environments*\n\n## Research Focus Areas\n\nThe collaboration will target three key technical challenges:\n\n1. **Adaptive Manufacturing**\n   - Real-time adaptation to product variations\n   - Learning from demonstration for quick reprogramming\n   - Quality control through multi-modal sensing\n\n2. **Safe Human-Robot Collaboration**\n   - Advanced proximity sensing and prediction\n   - Variable impedance control for safety\n   - Intuitive interfaces for non-experts\n\n3. **System Integration**\n   - Seamless integration with existing factory systems\n   - Fleet coordination for multi-robot workflows\n   - Digital twin technologies for simulation and planning\n\n### Partnership Structure\n\n| Component | Details |\n|-----------|---------|\n| Funding | $1.8M over 3 years |\n| Team | 4 PhD students, 2 postdocs, faculty advisors |\n| Industrial Partners | Robotics Frontiers R&D team |\n| IP Agreement | Jointly owned patents, open-source core algorithms |\n| Deployment Testbed | Robotics Frontiers pilot manufacturing facility |\n\n## Benefits to Research and Education\n\nThis partnership brings significant advantages to our academic mission:\n\n- **Real-world validation** of research in operational manufacturing environments\n- **Internship opportunities** for undergraduate and graduate students\n- **Industry-relevant problems** driving fundamental research questions\n- **Accelerated technology transfer** from lab to commercial applications\n\n> \"This partnership represents a perfect alignment of academic research excellence and industrial expertise. Together, we'll tackle fundamental challenges in collaborative robotics while developing solutions that can be deployed in real manufacturing settings today.\" ‚Äî Prof. Michael Torres, EMBER Lab Director\n\n## First Research Project: Adaptive Assembly\n\nThe initial project under this partnership will focus on developing a robotic system capable of assembling consumer electronics with high variability in components. This challenging task requires:\n\n* Precise manipulation of small parts\n* Adaptation to component variations\n* Quality verification through visual and tactile sensing\n* Learning from human demonstrations\n\n![Adaptive Assembly Demonstration](/images/news/berkeley-logo.png)\n*Prototype of our adaptive assembly system being tested with smartphone components*\n\n## Future Collaboration Opportunities\n\nWe will be hosting an **annual workshop** bringing together researchers and practitioners from academia and industry to discuss challenges and opportunities in collaborative robotics for manufacturing. The first workshop is scheduled for **Spring 2024**.\n\n---\n\n*For partnership inquiries, please contact our industry liaison office at partnerships@ember-lab.edu*","src/content/news/2023-10-12-partnershit.md","6f5ce0a7c17327ca",{html:259,metadata:260},"<h1 id=\"strategic-partnership-with-industry-leader-robotics-frontiers-inc\">Strategic Partnership with Industry Leader Robotics Frontiers Inc.</h1>\n<p>We are excited to announce a major <strong>3-year research partnership</strong> between the EMBER Lab and <strong>Robotics Frontiers Inc.</strong>, a leader in industrial automation and collaborative robotics. This partnership, valued at $1.8 million, will accelerate the development and deployment of next-generation collaborative robots for advanced manufacturing.</p>\n<p align=\"center\">\n  <img src=\"/images/news/berkeley-logo.png\" alt=\"EMBER Lab and Robotics Frontiers Partnership\" width=\"250\" style=\"height:auto;\">\n</p>\n<p><em>The partnership will focus on developing robots that can work safely alongside human workers in complex manufacturing environments</em></p>\n<h2 id=\"research-focus-areas\">Research Focus Areas</h2>\n<p>The collaboration will target three key technical challenges:</p>\n<ol>\n<li>\n<p><strong>Adaptive Manufacturing</strong></p>\n<ul>\n<li>Real-time adaptation to product variations</li>\n<li>Learning from demonstration for quick reprogramming</li>\n<li>Quality control through multi-modal sensing</li>\n</ul>\n</li>\n<li>\n<p><strong>Safe Human-Robot Collaboration</strong></p>\n<ul>\n<li>Advanced proximity sensing and prediction</li>\n<li>Variable impedance control for safety</li>\n<li>Intuitive interfaces for non-experts</li>\n</ul>\n</li>\n<li>\n<p><strong>System Integration</strong></p>\n<ul>\n<li>Seamless integration with existing factory systems</li>\n<li>Fleet coordination for multi-robot workflows</li>\n<li>Digital twin technologies for simulation and planning</li>\n</ul>\n</li>\n</ol>\n<h3 id=\"partnership-structure\">Partnership Structure</h3>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<table><thead><tr><th>Component</th><th>Details</th></tr></thead><tbody><tr><td>Funding</td><td>$1.8M over 3 years</td></tr><tr><td>Team</td><td>4 PhD students, 2 postdocs, faculty advisors</td></tr><tr><td>Industrial Partners</td><td>Robotics Frontiers R&#x26;D team</td></tr><tr><td>IP Agreement</td><td>Jointly owned patents, open-source core algorithms</td></tr><tr><td>Deployment Testbed</td><td>Robotics Frontiers pilot manufacturing facility</td></tr></tbody></table>\n<h2 id=\"benefits-to-research-and-education\">Benefits to Research and Education</h2>\n<p>This partnership brings significant advantages to our academic mission:</p>\n<ul>\n<li><strong>Real-world validation</strong> of research in operational manufacturing environments</li>\n<li><strong>Internship opportunities</strong> for undergraduate and graduate students</li>\n<li><strong>Industry-relevant problems</strong> driving fundamental research questions</li>\n<li><strong>Accelerated technology transfer</strong> from lab to commercial applications</li>\n</ul>\n<blockquote>\n<p>‚ÄúThis partnership represents a perfect alignment of academic research excellence and industrial expertise. Together, we‚Äôll tackle fundamental challenges in collaborative robotics while developing solutions that can be deployed in real manufacturing settings today.‚Äù ‚Äî Prof. Michael Torres, EMBER Lab Director</p>\n</blockquote>\n<h2 id=\"first-research-project-adaptive-assembly\">First Research Project: Adaptive Assembly</h2>\n<p>The initial project under this partnership will focus on developing a robotic system capable of assembling consumer electronics with high variability in components. This challenging task requires:</p>\n<ul>\n<li>Precise manipulation of small parts</li>\n<li>Adaptation to component variations</li>\n<li>Quality verification through visual and tactile sensing</li>\n<li>Learning from human demonstrations</li>\n</ul>\n<p><img src=\"/images/news/berkeley-logo.png\" alt=\"Adaptive Assembly Demonstration\">\n<em>Prototype of our adaptive assembly system being tested with smartphone components</em></p>\n<h2 id=\"future-collaboration-opportunities\">Future Collaboration Opportunities</h2>\n<p>We will be hosting an <strong>annual workshop</strong> bringing together researchers and practitioners from academia and industry to discuss challenges and opportunities in collaborative robotics for manufacturing. The first workshop is scheduled for <strong>Spring 2024</strong>.</p>\n<hr>\n<p><em>For partnership inquiries, please contact our industry liaison office at <a href=\"mailto:partnerships@ember-lab.edu\">partnerships@ember-lab.edu</a></em></p>",{headings:261,localImagePaths:280,remoteImagePaths:281,frontmatter:282,imagePaths:283},[262,265,268,271,274,277],{depth:203,slug:263,text:264},"strategic-partnership-with-industry-leader-robotics-frontiers-inc","Strategic Partnership with Industry Leader Robotics Frontiers Inc.",{depth:207,slug:266,text:267},"research-focus-areas","Research Focus Areas",{depth:211,slug:269,text:270},"partnership-structure","Partnership Structure",{depth:207,slug:272,text:273},"benefits-to-research-and-education","Benefits to Research and Education",{depth:207,slug:275,text:276},"first-research-project-adaptive-assembly","First Research Project: Adaptive Assembly",{depth:207,slug:278,text:279},"future-collaboration-opportunities","Future Collaboration Opportunities",[],[],{slug:249,title:252,date:253,summary:254},[],"nsf-grant-manipulation-research",{id:284,data:286,body:290,filePath:291,digest:292,rendered:293},{slug:284,title:287,date:288,summary:289},"EMBER Lab Receives $2.5M NSF Grant for Robotic Manipulation Research","2025-04-01","Our lab has been awarded a prestigious National Science Foundation grant to develop new techniques for robotic manipulation in unstructured environments.","# Major NSF Grant to Support Next-Generation Robotic Manipulation\n\nWe are thrilled to announce that the EMBER Lab has been awarded a **$2.5 million grant** from the National Science Foundation (NSF) through their National Robotics Initiative 3.0 program. This four-year project, titled \"_Robust Manipulation in Unstructured Environments through Multi-Modal Perception and Learning_,\" will push the boundaries of robotic manipulation capabilities.\n\n<p align=\"center\">\n  <img src=\"/images/news/nsf_logo.png\" alt=\"National Science Foundation Logo\" width=\"150\" style=\"height:auto;\">\n</p>\n\n*The project will leverage our lab's expertise in reinforcement learning and tactile sensing*\n\n## Research Objectives\n\nThis grant will support our team's efforts to develop:\n\n1. **Novel Tactile Sensors**\n   - High-resolution force distribution mapping\n   - Integration with compliant grippers\n   - Low-cost, robust design for practical deployment\n\n2. **Multi-Modal Learning Frameworks**\n   - Vision-touch integrated perception\n   - Self-supervised representation learning\n   - Few-shot adaptation to new objects\n\n3. **Real-World Deployment**\n   - Household assistance tasks\n   - Warehouse automation applications\n   - Human-robot collaborative manipulation\n\n### Project Team\n\nThe interdisciplinary research team includes:\n\n- **Principal Investigator**: Prof. Sarah Chen, EMBER Lab Director\n- **Co-PI**: Prof. David Rodriguez, Computer Vision\n- **Co-PI**: Prof. Michael Zhang, Machine Learning\n- **Collaborators**: Robotics Institute at Carnegie Mellon University\n\n## Research Impact and Broader Outcomes\n\n> \"This project addresses a fundamental challenge in robotics: enabling robots to manipulate previously unseen objects in unstructured environments. The techniques we develop will have applications ranging from assistive technologies to manufacturing.\" ‚Äî Prof. Sarah Chen\n\n### Grant Details\n\n| Category | Information |\n|----------|-------------|\n| Award Number | NSF-2235791 |\n| Duration | 4 years (2023-2027) |\n| Total Funding | $2,500,000 |\n| Graduate Students Supported | 5 |\n| Postdoctoral Researchers | 2 |\n\nThe funding will support:\n\n* Development of next-generation tactile sensors\n* Creation of a large-scale dataset of manipulation interactions\n* Open-source software frameworks for robotics researchers\n* Educational workshops for K-12 students\n\n## Educational Initiatives\n\nAs part of our broader impact activities, we will host:\n\n1. Annual summer workshops for high school students\n2. Open lab days for the community\n3. Research opportunities for undergraduate students\n\n![Student Workshop](/images/news/berkeley-logo.png)\n*High school students participating in our previous robotics workshop*\n\n---\n\n*For more information about this project or collaboration opportunities, please contact ember-lab@university.edu*","src/content/news/2025-04-01-new-grant.md","e0cd25e4857ce9a9",{html:294,metadata:295},"<h1 id=\"major-nsf-grant-to-support-next-generation-robotic-manipulation\">Major NSF Grant to Support Next-Generation Robotic Manipulation</h1>\n<p>We are thrilled to announce that the EMBER Lab has been awarded a <strong>$2.5 million grant</strong> from the National Science Foundation (NSF) through their National Robotics Initiative 3.0 program. This four-year project, titled ‚Äú<em>Robust Manipulation in Unstructured Environments through Multi-Modal Perception and Learning</em>,‚Äù will push the boundaries of robotic manipulation capabilities.</p>\n<p align=\"center\">\n  <img src=\"/images/news/nsf_logo.png\" alt=\"National Science Foundation Logo\" width=\"150\" style=\"height:auto;\">\n</p>\n<p><em>The project will leverage our lab‚Äôs expertise in reinforcement learning and tactile sensing</em></p>\n<h2 id=\"research-objectives\">Research Objectives</h2>\n<p>This grant will support our team‚Äôs efforts to develop:</p>\n<ol>\n<li>\n<p><strong>Novel Tactile Sensors</strong></p>\n<ul>\n<li>High-resolution force distribution mapping</li>\n<li>Integration with compliant grippers</li>\n<li>Low-cost, robust design for practical deployment</li>\n</ul>\n</li>\n<li>\n<p><strong>Multi-Modal Learning Frameworks</strong></p>\n<ul>\n<li>Vision-touch integrated perception</li>\n<li>Self-supervised representation learning</li>\n<li>Few-shot adaptation to new objects</li>\n</ul>\n</li>\n<li>\n<p><strong>Real-World Deployment</strong></p>\n<ul>\n<li>Household assistance tasks</li>\n<li>Warehouse automation applications</li>\n<li>Human-robot collaborative manipulation</li>\n</ul>\n</li>\n</ol>\n<h3 id=\"project-team\">Project Team</h3>\n<p>The interdisciplinary research team includes:</p>\n<ul>\n<li><strong>Principal Investigator</strong>: Prof. Sarah Chen, EMBER Lab Director</li>\n<li><strong>Co-PI</strong>: Prof. David Rodriguez, Computer Vision</li>\n<li><strong>Co-PI</strong>: Prof. Michael Zhang, Machine Learning</li>\n<li><strong>Collaborators</strong>: Robotics Institute at Carnegie Mellon University</li>\n</ul>\n<h2 id=\"research-impact-and-broader-outcomes\">Research Impact and Broader Outcomes</h2>\n<blockquote>\n<p>‚ÄúThis project addresses a fundamental challenge in robotics: enabling robots to manipulate previously unseen objects in unstructured environments. The techniques we develop will have applications ranging from assistive technologies to manufacturing.‚Äù ‚Äî Prof. Sarah Chen</p>\n</blockquote>\n<h3 id=\"grant-details\">Grant Details</h3>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<table><thead><tr><th>Category</th><th>Information</th></tr></thead><tbody><tr><td>Award Number</td><td>NSF-2235791</td></tr><tr><td>Duration</td><td>4 years (2023-2027)</td></tr><tr><td>Total Funding</td><td>$2,500,000</td></tr><tr><td>Graduate Students Supported</td><td>5</td></tr><tr><td>Postdoctoral Researchers</td><td>2</td></tr></tbody></table>\n<p>The funding will support:</p>\n<ul>\n<li>Development of next-generation tactile sensors</li>\n<li>Creation of a large-scale dataset of manipulation interactions</li>\n<li>Open-source software frameworks for robotics researchers</li>\n<li>Educational workshops for K-12 students</li>\n</ul>\n<h2 id=\"educational-initiatives\">Educational Initiatives</h2>\n<p>As part of our broader impact activities, we will host:</p>\n<ol>\n<li>Annual summer workshops for high school students</li>\n<li>Open lab days for the community</li>\n<li>Research opportunities for undergraduate students</li>\n</ol>\n<p><img src=\"/images/news/berkeley-logo.png\" alt=\"Student Workshop\">\n<em>High school students participating in our previous robotics workshop</em></p>\n<hr>\n<p><em>For more information about this project or collaboration opportunities, please contact <a href=\"mailto:ember-lab@university.edu\">ember-lab@university.edu</a></em></p>",{headings:296,localImagePaths:315,remoteImagePaths:316,frontmatter:317,imagePaths:318},[297,300,303,306,309,312],{depth:203,slug:298,text:299},"major-nsf-grant-to-support-next-generation-robotic-manipulation","Major NSF Grant to Support Next-Generation Robotic Manipulation",{depth:207,slug:301,text:302},"research-objectives","Research Objectives",{depth:211,slug:304,text:305},"project-team","Project Team",{depth:207,slug:307,text:308},"research-impact-and-broader-outcomes","Research Impact and Broader Outcomes",{depth:211,slug:310,text:311},"grant-details","Grant Details",{depth:207,slug:313,text:314},"educational-initiatives","Educational Initiatives",[],[],{slug:284,title:287,date:288,summary:289},[],"research",["Map",321,322,353,354,382,383,410,411],"human-robot",{id:321,data:323,body:335,filePath:336,digest:337,rendered:338},{title:324,slug:321,order:203,summary:325,homepage_image:326,featured:18,images:327,key_papers:334},"Human-Robot Collaboration","Developing robots that can work alongside humans in shared workspaces, interpreting human gestures and intentions.","/images/research/human-robot.webp",[328,331],{url:326,alt:329,caption:330},"Robot collaborating with human in a shared workspace","Our robots learn to interpret human gestures and intentions",{url:326,alt:332,caption:333},"Robot assisting in manufacturing","Collaborative robots in manufacturing environments",[11,34],"Our research in Human-Robot Collaboration focuses on enabling robots to work effectively alongside humans in shared spaces. This requires robots to understand human intentions, communicate their own plans, and adapt to changing environments in real-time.\n\n## Key Research Directions\n \n- **Intention recognition**: Developing models that can infer human goals from partial observations and natural movements\n- **Adaptive collaboration**: Creating frameworks for robots to adjust their behavior based on human preferences and work styles\n- **Safety-aware planning**: Designing motion planning algorithms that prioritize human safety while maintaining task efficiency\n- **Intuitive interfaces**: Building communication channels that allow for seamless human-robot interaction without specialized training\n\nThrough this work, we aim to create robotic systems that can be deployed in environments ranging from manufacturing floors to healthcare settings, augmenting human capabilities rather than replacing them.\n\nOur latest projects involve using multimodal learning to integrate visual, tactile, and linguistic cues for more natural human-robot interaction. We're particularly excited about our recent advances in gesture-based teaching, which allows non-experts to quickly program robots for new tasks.","src/content/research/human-robot.md","34e307305d504c42",{html:339,metadata:340},"<p>Our research in Human-Robot Collaboration focuses on enabling robots to work effectively alongside humans in shared spaces. This requires robots to understand human intentions, communicate their own plans, and adapt to changing environments in real-time.</p>\n<h2 id=\"key-research-directions\">Key Research Directions</h2>\n<ul>\n<li><strong>Intention recognition</strong>: Developing models that can infer human goals from partial observations and natural movements</li>\n<li><strong>Adaptive collaboration</strong>: Creating frameworks for robots to adjust their behavior based on human preferences and work styles</li>\n<li><strong>Safety-aware planning</strong>: Designing motion planning algorithms that prioritize human safety while maintaining task efficiency</li>\n<li><strong>Intuitive interfaces</strong>: Building communication channels that allow for seamless human-robot interaction without specialized training</li>\n</ul>\n<p>Through this work, we aim to create robotic systems that can be deployed in environments ranging from manufacturing floors to healthcare settings, augmenting human capabilities rather than replacing them.</p>\n<p>Our latest projects involve using multimodal learning to integrate visual, tactile, and linguistic cues for more natural human-robot interaction. We‚Äôre particularly excited about our recent advances in gesture-based teaching, which allows non-experts to quickly program robots for new tasks.</p>",{headings:341,localImagePaths:345,remoteImagePaths:346,frontmatter:347,imagePaths:352},[342],{depth:207,slug:343,text:344},"key-research-directions","Key Research Directions",[],[],{title:324,slug:321,order:203,summary:325,homepage_image:326,featured:18,images:348,key_papers:351},[349,350],{url:326,alt:329,caption:330},{url:326,alt:332,caption:333},[11,34],[],"manipulation",{id:353,data:355,body:365,filePath:366,digest:367,rendered:368},{title:356,slug:353,order:357,summary:358,homepage_image:359,featured:18,images:360,key_papers:364},"Robotic Manipulation",4,"Enabling robots to manipulate objects with human-like dexterity through advanced control and learning algorithms.","/images/research/dex.png",[361],{url:359,alt:362,caption:363},"Robot hand manipulating an object","Dexterous manipulation of everyday objects",[11,34],"Our research in robotic manipulation focuses on developing algorithms and systems that allow robots to interact with objects in their environment with human-like dexterity. We work on both grasping and in-hand manipulation, with a focus on generalizable skills that can be applied to a wide range of objects and tasks.\n\n## Research Directions\n\n- **Learning from demonstration**: Developing techniques for robots to learn manipulation skills from human demonstrations\n- **Tactile sensing**: Integrating rich tactile feedback to enable more precise and adaptive manipulation\n- **Multi-fingered control**: Creating control algorithms for complex, multi-fingered robotic hands\n- **Visual-tactile integration**: Combining visual and tactile information for robust perception during manipulation tasks\n\nOne key aspect of our approach is combining model-based control techniques with learning-based methods. This hybrid approach allows us to leverage physical intuition while still capturing the complexity of contact dynamics during manipulation.\n\nOur lab is equipped with several robotic manipulators, including custom-designed hands and commercially available systems. We test our algorithms on a wide range of everyday objects, working toward general-purpose manipulation capabilities that can support applications from manufacturing to assistive robotics.","src/content/research/manipulation.md","270535f4c5b37adc",{html:369,metadata:370},"<p>Our research in robotic manipulation focuses on developing algorithms and systems that allow robots to interact with objects in their environment with human-like dexterity. We work on both grasping and in-hand manipulation, with a focus on generalizable skills that can be applied to a wide range of objects and tasks.</p>\n<h2 id=\"research-directions\">Research Directions</h2>\n<ul>\n<li><strong>Learning from demonstration</strong>: Developing techniques for robots to learn manipulation skills from human demonstrations</li>\n<li><strong>Tactile sensing</strong>: Integrating rich tactile feedback to enable more precise and adaptive manipulation</li>\n<li><strong>Multi-fingered control</strong>: Creating control algorithms for complex, multi-fingered robotic hands</li>\n<li><strong>Visual-tactile integration</strong>: Combining visual and tactile information for robust perception during manipulation tasks</li>\n</ul>\n<p>One key aspect of our approach is combining model-based control techniques with learning-based methods. This hybrid approach allows us to leverage physical intuition while still capturing the complexity of contact dynamics during manipulation.</p>\n<p>Our lab is equipped with several robotic manipulators, including custom-designed hands and commercially available systems. We test our algorithms on a wide range of everyday objects, working toward general-purpose manipulation capabilities that can support applications from manufacturing to assistive robotics.</p>",{headings:371,localImagePaths:375,remoteImagePaths:376,frontmatter:377,imagePaths:381},[372],{depth:207,slug:373,text:374},"research-directions","Research Directions",[],[],{title:356,slug:353,order:357,summary:358,homepage_image:359,featured:18,images:378,key_papers:380},[379],{url:359,alt:362,caption:363},[11,34],[],"racing",{id:382,data:384,body:393,filePath:394,digest:395,rendered:396},{title:385,slug:382,order:207,summary:386,homepage_image:387,featured:18,images:388,key_papers:392},"Autonomous Racing","Creating algorithms for high-speed autonomous racing vehicles that can navigate complex tracks at the limits of control.","/images/research/ces_2025_ai_racing.webp",[389],{url:387,alt:390,caption:391},"Autonomous racing vehicle on a track","Our autonomous racing system competing in the CES 2025 AI Racing League",[11,34],"Our autonomous racing research focuses on developing algorithms that enable vehicles to navigate complex tracks at high speeds, often at the limits of friction and control. This research area combines optimal control theory, machine learning, and vehicle dynamics to create racing systems that can outperform human drivers.\n\n## Research Challenges\n\n- **Model predictive control at the limits**: Developing control algorithms that can handle the nonlinear dynamics of vehicles operating at the friction limits\n- **Adversarial learning for robust control**: Creating policies that are robust to variations in track conditions, vehicle parameters, and disturbances\n- **Multi-agent racing strategies**: Enabling vehicles to make strategic decisions when competing against other autonomous or human-driven vehicles\n- **Sim-to-real transfer**: Bridging the gap between simulation and real-world performance for high-speed racing scenarios\n\nOur lab has developed a novel framework called DiAL-MPC (Dynamic Adversarial Learning for Model Predictive Control) that combines the strengths of model-based control with learning-based approaches. This approach has led to significant improvements in racing performance across a variety of track configurations.\n\nWe collaborate closely with industry partners and regularly participate in autonomous racing competitions, including the CES AI Racing League and the Indy Autonomous Challenge, where our algorithms have consistently achieved top performances.","src/content/research/racing.md","6b2a6a652ef35136",{html:397,metadata:398},"<p>Our autonomous racing research focuses on developing algorithms that enable vehicles to navigate complex tracks at high speeds, often at the limits of friction and control. This research area combines optimal control theory, machine learning, and vehicle dynamics to create racing systems that can outperform human drivers.</p>\n<h2 id=\"research-challenges\">Research Challenges</h2>\n<ul>\n<li><strong>Model predictive control at the limits</strong>: Developing control algorithms that can handle the nonlinear dynamics of vehicles operating at the friction limits</li>\n<li><strong>Adversarial learning for robust control</strong>: Creating policies that are robust to variations in track conditions, vehicle parameters, and disturbances</li>\n<li><strong>Multi-agent racing strategies</strong>: Enabling vehicles to make strategic decisions when competing against other autonomous or human-driven vehicles</li>\n<li><strong>Sim-to-real transfer</strong>: Bridging the gap between simulation and real-world performance for high-speed racing scenarios</li>\n</ul>\n<p>Our lab has developed a novel framework called DiAL-MPC (Dynamic Adversarial Learning for Model Predictive Control) that combines the strengths of model-based control with learning-based approaches. This approach has led to significant improvements in racing performance across a variety of track configurations.</p>\n<p>We collaborate closely with industry partners and regularly participate in autonomous racing competitions, including the CES AI Racing League and the Indy Autonomous Challenge, where our algorithms have consistently achieved top performances.</p>",{headings:399,localImagePaths:403,remoteImagePaths:404,frontmatter:405,imagePaths:409},[400],{depth:207,slug:401,text:402},"research-challenges","Research Challenges",[],[],{title:385,slug:382,order:207,summary:386,homepage_image:387,featured:18,images:406,key_papers:408},[407],{url:387,alt:390,caption:391},[11,34],[],"self-play",{id:410,data:412,body:421,filePath:422,digest:423,rendered:424},{title:413,slug:410,order:211,summary:414,homepage_image:415,featured:18,images:416,key_papers:420},"Self-Play Reinforcement Learning","Training robotic systems through self-play, allowing them to discover optimal strategies through competition.","/images/research/self_play.png",[417],{url:415,alt:418,caption:419},"Self-play reinforcement learning visualization","Visualization of robots learning through self-competition",[11,34],"Self-play reinforcement learning allows robotic systems to evolve sophisticated behaviors by competing against themselves or previous versions. This approach has led to breakthrough results in complex domains like game playing (AlphaGo, OpenAI Five) and we're extending these techniques to physical robotics problems.\n\n## Key Research Areas\n\n- **Multi-agent competition**: Developing frameworks where multiple agents can compete and collaboratively improve their policies\n- **Curriculum learning through self-play**: Creating progressively more challenging scenarios as the agent improves\n- **Emergent behaviors**: Studying the unexpected strategies and behaviors that emerge from self-play training\n- **Transfer to real robots**: Ensuring that behaviors learned in simulation transfer effectively to physical systems\n\nOur recent work has shown that self-play can be particularly effective for tasks where the optimal behavior is difficult to specify through traditional reward engineering. By allowing the system to discover its own solutions through competitive optimization, we can achieve more robust and adaptive behaviors.\n\nWe apply these techniques to a range of robotic applications, from dexterous manipulation to multi-robot coordination problems, advancing the frontiers of autonomous learning systems.","src/content/research/self-play.md","70e9fbcb059194fb",{html:425,metadata:426},"<p>Self-play reinforcement learning allows robotic systems to evolve sophisticated behaviors by competing against themselves or previous versions. This approach has led to breakthrough results in complex domains like game playing (AlphaGo, OpenAI Five) and we‚Äôre extending these techniques to physical robotics problems.</p>\n<h2 id=\"key-research-areas\">Key Research Areas</h2>\n<ul>\n<li><strong>Multi-agent competition</strong>: Developing frameworks where multiple agents can compete and collaboratively improve their policies</li>\n<li><strong>Curriculum learning through self-play</strong>: Creating progressively more challenging scenarios as the agent improves</li>\n<li><strong>Emergent behaviors</strong>: Studying the unexpected strategies and behaviors that emerge from self-play training</li>\n<li><strong>Transfer to real robots</strong>: Ensuring that behaviors learned in simulation transfer effectively to physical systems</li>\n</ul>\n<p>Our recent work has shown that self-play can be particularly effective for tasks where the optimal behavior is difficult to specify through traditional reward engineering. By allowing the system to discover its own solutions through competitive optimization, we can achieve more robust and adaptive behaviors.</p>\n<p>We apply these techniques to a range of robotic applications, from dexterous manipulation to multi-robot coordination problems, advancing the frontiers of autonomous learning systems.</p>",{headings:427,localImagePaths:431,remoteImagePaths:432,frontmatter:433,imagePaths:437},[428],{depth:207,slug:429,text:430},"key-research-areas","Key Research Areas",[],[],{title:413,slug:410,order:211,summary:414,homepage_image:415,featured:18,images:434,key_papers:436},[435],{url:415,alt:418,caption:419},[11,34],[],"contact_page",["Map",140,440],{id:140,data:441,body:443,filePath:444,digest:445,rendered:446},{title:442,slug:140},"Join the EMBER Lab","# Join the EMBER Lab at UC Berkeley\n\nThank you for your interest in the **EMBER Lab** (*Energy, Materials, and Bioenergy Research*) at UC Berkeley! We are seeking enthusiastic researchers at all levels to help advance our work in sustainable energy solutions.\n\n## Who We're Looking For\n\n- **Postdoctoral Researchers**\n- **PhD Students**\n- **Master's Students**\n- **Undergraduate Researchers**\n- **Visiting Scholars**\n\n---\n\n### Prospective PhD Students\n\nWe accept PhD students through multiple departments at UC Berkeley:\n1. [Materials Science & Engineering](https://mse.berkeley.edu)\n2. [Chemical Engineering](https://chemistry.berkeley.edu)\n3. [Energy & Resources Group](https://erg.berkeley.edu)\n\n> **Note:** Please mention the EMBER Lab in your application statement. While we review all applications, we particularly value candidates with prior research experience in sustainable energy.\n\n### Current Berkeley Students\n\nIf you're already at Berkeley and interested in joining our lab:\n\n| Student Level | How to Apply |\n|---------------|--------------|\n| PhD | Email Prof. Chen directly with CV |\n| Masters | Complete our [lab interest form](#) |\n| Undergraduate | Attend our info session each semester |\n\n### Undergraduate Research Opportunities\n\nWe offer several undergraduate research positions each semester. Requirements:\n* Minimum 3.2 GPA in relevant coursework\n* Commitment of at least 10 hours/week\n* Previous lab experience preferred but not required\n\n*For more information, please visit our [undergraduate opportunities page](https://example.berkeley.edu/ember/undergrad).*\n\n## Current Research Areas\n\nOur lab focuses on:\n\n1. **Bioenergy conversion technologies**\n2. **Sustainable materials design**\n3. **Energy storage solutions**\n4. **Catalysis for renewable fuels**\n\n\n## Contact Us\n\nFor any questions about joining the lab, please email us at [ember-lab@berkeley.edu](mailto:ember-lab@berkeley.edu) with your CV and a brief statement of research interests.\n\n***The best science happens through collaboration and diversity of thought. We are committed to building an inclusive research environment.***","src/content/contact-page.md","5ddb48ee734ab301",{html:447,metadata:448},"<h1 id=\"join-the-ember-lab-at-uc-berkeley\">Join the EMBER Lab at UC Berkeley</h1>\n<p>Thank you for your interest in the <strong>EMBER Lab</strong> (<em>Energy, Materials, and Bioenergy Research</em>) at UC Berkeley! We are seeking enthusiastic researchers at all levels to help advance our work in sustainable energy solutions.</p>\n<h2 id=\"who-were-looking-for\">Who We‚Äôre Looking For</h2>\n<ul>\n<li><strong>Postdoctoral Researchers</strong></li>\n<li><strong>PhD Students</strong></li>\n<li><strong>Master‚Äôs Students</strong></li>\n<li><strong>Undergraduate Researchers</strong></li>\n<li><strong>Visiting Scholars</strong></li>\n</ul>\n<hr>\n<h3 id=\"prospective-phd-students\">Prospective PhD Students</h3>\n<p>We accept PhD students through multiple departments at UC Berkeley:</p>\n<ol>\n<li><a href=\"https://mse.berkeley.edu\">Materials Science &#x26; Engineering</a></li>\n<li><a href=\"https://chemistry.berkeley.edu\">Chemical Engineering</a></li>\n<li><a href=\"https://erg.berkeley.edu\">Energy &#x26; Resources Group</a></li>\n</ol>\n<blockquote>\n<p><strong>Note:</strong> Please mention the EMBER Lab in your application statement. While we review all applications, we particularly value candidates with prior research experience in sustainable energy.</p>\n</blockquote>\n<h3 id=\"current-berkeley-students\">Current Berkeley Students</h3>\n<p>If you‚Äôre already at Berkeley and interested in joining our lab:</p>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<table><thead><tr><th>Student Level</th><th>How to Apply</th></tr></thead><tbody><tr><td>PhD</td><td>Email Prof. Chen directly with CV</td></tr><tr><td>Masters</td><td>Complete our <a href=\"#\">lab interest form</a></td></tr><tr><td>Undergraduate</td><td>Attend our info session each semester</td></tr></tbody></table>\n<h3 id=\"undergraduate-research-opportunities\">Undergraduate Research Opportunities</h3>\n<p>We offer several undergraduate research positions each semester. Requirements:</p>\n<ul>\n<li>Minimum 3.2 GPA in relevant coursework</li>\n<li>Commitment of at least 10 hours/week</li>\n<li>Previous lab experience preferred but not required</li>\n</ul>\n<p><em>For more information, please visit our <a href=\"https://example.berkeley.edu/ember/undergrad\">undergraduate opportunities page</a>.</em></p>\n<h2 id=\"current-research-areas\">Current Research Areas</h2>\n<p>Our lab focuses on:</p>\n<ol>\n<li><strong>Bioenergy conversion technologies</strong></li>\n<li><strong>Sustainable materials design</strong></li>\n<li><strong>Energy storage solutions</strong></li>\n<li><strong>Catalysis for renewable fuels</strong></li>\n</ol>\n<h2 id=\"contact-us\">Contact Us</h2>\n<p>For any questions about joining the lab, please email us at <a href=\"mailto:ember-lab@berkeley.edu\">ember-lab@berkeley.edu</a> with your CV and a brief statement of research interests.</p>\n<p><em><strong>The best science happens through collaboration and diversity of thought. We are committed to building an inclusive research environment.</strong></em></p>",{headings:449,localImagePaths:471,remoteImagePaths:472,frontmatter:441,imagePaths:473},[450,453,456,459,462,465,468],{depth:203,slug:451,text:452},"join-the-ember-lab-at-uc-berkeley","Join the EMBER Lab at UC Berkeley",{depth:207,slug:454,text:455},"who-were-looking-for","Who We‚Äôre Looking For",{depth:211,slug:457,text:458},"prospective-phd-students","Prospective PhD Students",{depth:211,slug:460,text:461},"current-berkeley-students","Current Berkeley Students",{depth:211,slug:463,text:464},"undergraduate-research-opportunities","Undergraduate Research Opportunities",{depth:207,slug:466,text:467},"current-research-areas","Current Research Areas",{depth:207,slug:469,text:470},"contact-us","Contact Us",[],[],[]];

export { _astro_dataLayerContent as default };
